import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from sklearn.metrics import (roc_curve, roc_auc_score, precision_recall_curve,
                             confusion_matrix, classification_report)
from sklearn.calibration import calibration_curve
import warnings
import json
import os
import re
from typing import Dict, Optional

warnings.filterwarnings('ignore')

# =============================
# üîê API KEY CONFIGURATION (SECURE METHOD)
# =============================
# ‚ö†Ô∏è NEVER hardcode API Keys in your code!
# Please use environment variables: export ANTHROPIC_API_KEY="your-key-here"
if "ANTHROPIC_API_KEY" not in os.environ:
    st.warning("‚ö†Ô∏è ANTHROPIC_API_KEY not set. Please set it as environment variable for AI parsing.")
    # If you need temporary testing, uncomment the line below and fill in the key (remember to remove before sharing code)
    #os.environ["ANTHROPIC_API_KEY"] = ""

# =============================
# GLOBAL CONSTANT DEFINITIONS (UPDATED FOR FAIR VERSION)
# =============================
ETHNICITIES_LIST = [
    'Australian', 'Aboriginal', 'Torres Strait Islander', 'British',
    'Indian', 'Chinese', 'New Zealand', 'Filipino',
    'Vietnamese', 'South African', 'Other'
]

DEGREE_MAJORS = ['Education', 'Commerce', 'STEM', 'Science', 'Arts', 'Other']
TEACHING_EXPERIENCE = ['<1 year', '1-3 years', '3+ years']
GENDERS = ['Male', 'Female', 'Non-binary']


# =============================
# LLM-BASED RESUME PARSING (CLAUDE AI)
# =============================
def extract_markdown_from_pdf_llm(pdf_file) -> str:
    """Use high-quality LLM parser to convert PDF to Markdown"""
    try:
        import pymupdf4llm
        import tempfile
        import os

        pdf_bytes = pdf_file.read()
        pdf_file.seek(0)

        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
            tmp_file.write(pdf_bytes)
            tmp_path = tmp_file.name

        try:
            md_text = pymupdf4llm.to_markdown(tmp_path)
            return md_text
        finally:
            if os.path.exists(tmp_path):
                os.unlink(tmp_path)

    except ImportError:
        st.warning("‚ö†Ô∏è pymupdf4llm not installed. Using PyMuPDF fallback...")
        try:
            import fitz
            pdf_bytes = pdf_file.read()
            pdf_file.seek(0)
            doc = fitz.open(stream=pdf_bytes, filetype="pdf")
            text = ""
            for page in doc:
                text += page.get_text()
            return text
        except ImportError:
            st.warning("‚ö†Ô∏è PyMuPDF not installed either. Will try PyPDF2...")
            return ""
    except Exception as e:
        st.error(f"‚ùå Error parsing PDF: {str(e)}")
        return ""


def extract_features_with_llm_claude(markdown_text: str) -> Dict:
    """Use Claude API to extract structured features from resume text"""
    try:
        import anthropic

        api_key = os.environ.get("ANTHROPIC_API_KEY")
        if not api_key or api_key == "your-api-key-here":
            st.error("‚ùå ANTHROPIC_API_KEY not configured. Using fallback parser...")
            return None

        client = anthropic.Anthropic(api_key=api_key)

        prompt = f"""You are analyzing a teaching candidate's resume for Teach For Australia. 

Extract the following information from this resume and return it as a valid JSON object. Use 4.0 GPA scale and 1-10 scale for other scores.

Required fields:
- cohort: (integer) Year they might join TFA, default to 2023
- gender: (string) One of: "Male", "Female", "Non-binary" - infer from name/pronouns, default "Female"
- ethnicity: (string) One of: "Australian", "Aboriginal", "Torres Strait Islander", "British", "Indian", "Chinese", "New Zealand", "Filipino", "Vietnamese", "South African", "Other"
- degree_major: (string) One of: "Education", "Commerce", "STEM", "Science", "Arts", "Other"
- gpa: (float) GPA on 2.0-4.0 scale. If percentages: 50-64%=2.0, 65-74%=2.5, 75-84%=3.0, 85-100%=4.0. If HD/D/C/P: HD=4.0, D=3.0, C=2.5, P=2.0
- teaching_experience: (string) One of: "<1 year", "1-3 years", "3+ years"
- leadership_score: (float) 1.0-10.0. Base on: leadership roles, team management, captain/president positions
- communication_score: (float) 1.0-10.0. Base on: presentations, publications, public speaking
- self_efficacy: (float) 1.0-10.0. Base on: achievements, awards, confident language
- organisational_support: (float) 1.0-10.0. Base on: alignment with education/social causes

Resume text:
{markdown_text[:8000]}

Return ONLY a valid JSON object with these exact keys, no other text."""

        message = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=1024,
            messages=[{"role": "user", "content": prompt}]
        )

        response_text = message.content[0].text
        json_start = response_text.find('{')
        json_end = response_text.rfind('}') + 1

        if json_start != -1 and json_end > json_start:
            json_str = response_text[json_start:json_end]
            extracted_data = json.loads(json_str)
            extracted_data['raw_text'] = markdown_text[:500]
            return extracted_data
        else:
            st.error("‚ùå Could not parse JSON from LLM response")
            return None

    except Exception as e:
        st.error(f"‚ùå Error calling Claude API: {str(e)}")
        return None


# =============================
# KEYWORD-BASED PARSING (FALLBACK)
# =============================
def extract_text_from_pdf(pdf_file):
    """Extract text using PyPDF2"""
    try:
        import PyPDF2
        import io

        pdf_bytes = pdf_file.read()
        pdf_file.seek(0)

        pdf_reader = PyPDF2.PdfReader(io.BytesIO(pdf_bytes))
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text()
        return text
    except ImportError:
        st.warning("‚ö†Ô∏è PyPDF2 not installed. Install with: pip install PyPDF2")
        return ""
    except Exception as e:
        st.error(f"‚ùå Error reading PDF: {str(e)}")
        return ""


def parse_resume_text_keywords(text):
    """Parse resume using keyword analysis (fallback method)"""
    extracted = {
        'cohort': 2023,
        'gender': "Female",
        'ethnicity': "Australian",
        'degree_major': "Other",
        'gpa': 2.5,
        'teaching_experience': "<1 year",
        'leadership_score': 6.0,
        'communication_score': 7.0,
        'self_efficacy': 6.8,
        'organisational_support': 5.5,
        'raw_text': text[:500]
    }

    text_lower = text.lower()

    # Extract GPA (4.0 scale)
    gpa_patterns = [
        r'gpa[:\s]+(\d+\.?\d*)',
        r'grade point average[:\s]+(\d+\.?\d*)',
        r'wam[:\s]+(\d+\.?\d*)',
        r'grade[:\s]+(\d+\.?\d*)',
    ]
    for pattern in gpa_patterns:
        match = re.search(pattern, text_lower)
        if match:
            gpa_val = float(match.group(1))
            if gpa_val > 4.0:  # Convert percentage to 4.0 scale
                if gpa_val >= 85:
                    gpa_val = 4.0
                elif gpa_val >= 75:
                    gpa_val = 3.0
                elif gpa_val >= 65:
                    gpa_val = 2.5
                else:
                    gpa_val = 2.0
            extracted['gpa'] = min(max(gpa_val, 2.0), 4.0)
            break

    # Extract major
    major_keywords = {
        'STEM': [
            'statistics', 'statistical', 'mathematics', 'mathematical',
            'engineering', 'computer', 'data science', 'analytics',
            'technology', 'software', 'information technology'
        ],
        'Science': [
            'science', 'physics', 'chemistry', 'biology'
        ],
        'Commerce': [
            'business', 'commerce', 'economics', 'finance', 'financial',
            'accounting', 'marketing', 'management', 'mba'
        ],
        'Education': [
            'bachelor of education', 'master of education',
            'teaching degree', 'pedagogy', 'curriculum'
        ],
        'Arts': [
            'humanities', 'history', 'philosophy', 'literature',
            'linguistics', 'languages', 'sociology', 'psychology',
            'fine arts', 'design', 'music', 'theatre',
            'visual arts', 'creative arts', 'media'
        ]
    }

    major_found = False
    for major, keywords in major_keywords.items():
        for keyword in keywords:
            pattern = r'\b' + re.escape(keyword) + r'\b'
            if re.search(pattern, text_lower):
                extracted['degree_major'] = major
                major_found = True
                break
        if major_found:
            break

    # Extract teaching experience
    teaching_indicators = ['teacher', 'teaching', 'tutor', 'educator', 'instructor', 'taught']
    has_teaching = any(word in text_lower for word in teaching_indicators)

    if has_teaching:
        exp_match = re.search(
            r'(\d+)\s*(?:\+)?\s*(?:years?|yrs?)\s*(?:of\s*)?(?:teaching|tutoring|education|experience)',
            text_lower
        )
        if exp_match:
            years = int(exp_match.group(1))
            if years >= 3:
                extracted['teaching_experience'] = "3+ years"
            elif years >= 1:
                extracted['teaching_experience'] = "1-3 years"
            else:
                extracted['teaching_experience'] = "<1 year"
        else:
            extracted['teaching_experience'] = "<1 year"

    # Infer scores (1-10 scale)
    leadership_keywords = [
        'lead', 'leader', 'leadership', 'manage', 'manager', 'management',
        'coordinate', 'coordinator', 'president', 'captain', 'director',
        'supervisor', 'head', 'chief', 'founded', 'organized'
    ]
    leadership_count = sum(1 for keyword in leadership_keywords if keyword in text_lower)
    extracted['leadership_score'] = min(4.0 + (leadership_count * 0.5), 9.5)

    communication_keywords = [
        'presentation', 'presented', 'public speaking', 'communication',
        'wrote', 'writing', 'published', 'publication', 'article',
        'report', 'blog', 'spoke', 'conference', 'seminar'
    ]
    comm_count = sum(1 for keyword in communication_keywords if keyword in text_lower)
    extracted['communication_score'] = min(5.0 + (comm_count * 0.5), 9.5)

    efficacy_keywords = [
        'confident', 'achieved', 'achievement', 'successful', 'success',
        'award', 'awarded', 'recognition', 'excellent', 'outstanding',
        'top', 'best', 'winner', 'first place', 'dean\'s list'
    ]
    efficacy_count = sum(1 for keyword in efficacy_keywords if keyword in text_lower)
    extracted['self_efficacy'] = min(5.0 + (efficacy_count * 0.5), 9.0)

    education_commitment = [
        'education', 'teach', 'student', 'learning', 'volunteer',
        'community', 'social impact', 'mentor', 'tutor'
    ]
    org_count = sum(1 for keyword in education_commitment if keyword in text_lower)
    extracted['organisational_support'] = min(4.0 + (org_count * 0.3), 9.0)

    return extracted


def parse_resume_hybrid(pdf_file, use_llm=True):
    """Hybrid parsing method: prioritize LLM, fall back to keywords on failure"""
    if use_llm:
        st.info("üìÑ Step 1/2: Converting PDF to text...")
        markdown_text = extract_markdown_from_pdf_llm(pdf_file)

        if markdown_text and len(markdown_text) > 50:
            st.success(f"‚úÖ Extracted {len(markdown_text)} characters")
            st.info("üìÑ Step 2/2: Analyzing with Claude AI...")
            extracted_features = extract_features_with_llm_claude(markdown_text)

            if extracted_features:
                st.success("‚úÖ Successfully extracted features with AI!")
                return extracted_features

        st.warning("‚ö†Ô∏è LLM parsing failed, using keyword-based fallback...")

    # Fallback to keyword-based parsing
    pdf_file.seek(0)
    text = extract_text_from_pdf(pdf_file)
    if text:
        return parse_resume_text_keywords(text)

    st.error("‚ùå Could not extract text from PDF")
    return None


# =============================
# PAGE CONFIG
# =============================
st.set_page_config(
    page_title="TFA AI Fairness Dashboard",
    page_icon="üéì",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: 700;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 0.5rem;
    }
    .sub-header {
        font-size: 1.2rem;
        color: #666;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1.5rem;
        border-radius: 10px;
        color: white;
        text-align: center;
    }
    .stTabs [data-baseweb="tab-list"] {
        gap: 2rem;
    }
    .stTabs [data-baseweb="tab"] {
        height: 3rem;
        padding: 0 2rem;
        font-size: 1.1rem;
    }
    .upload-section {
        background: #f8f9fa;
        padding: 1.5rem;
        border-radius: 10px;
        border: 2px dashed #1f77b4;
        margin: 1rem 0;
    }
    .fasttrack-box {
        background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
        padding: 2rem;
        border-radius: 15px;
        color: white;
        text-align: center;
        font-size: 2rem;
        font-weight: bold;
        margin: 1rem 0;
    }
    .secondlook-box {
        background: linear-gradient(135deg, #ee0979 0%, #ff6a00 100%);
        padding: 2rem;
        border-radius: 15px;
        color: white;
        text-align: center;
        font-size: 2rem;
        font-weight: bold;
        margin: 1rem 0;
    }
    .info-box {
        background: #f0f8ff;
        padding: 1.5rem;
        border-radius: 10px;
        border-left: 5px solid #1f77b4;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

st.markdown('<p class="main-header">üéì Teach For Australia</p>', unsafe_allow_html=True)
st.markdown('<p class="sub-header">AI-Powered Fair Recruitment Dashboard (GPA: 4.0 Scale)</p>',
            unsafe_allow_html=True)


# =============================
# DATA LOADING (FAIR VERSION DATA GENERATION)
# =============================
@st.cache_data
def load_data():
    file_path = "teacher_hiring_fair.csv"
    should_generate = False

    if os.path.exists(file_path):
        try:
            df = pd.read_csv(file_path)
            required_columns = ['candidate_id', 'cohort', 'gender', 'ethnicity', 'degree_major',
                                'teaching_experience', 'gpa', 'leadership_score',
                                'communication_score', 'self_efficacy', 'organisational_support',
                                'selected', 'proba']
            missing_columns = [col for col in required_columns if col not in df.columns]

            if missing_columns:
                st.warning(f"‚ö†Ô∏è Missing columns: {', '.join(missing_columns)}")
                should_generate = True
            else:
                # Check for old column name formats
                old_columns = ['gpa_numeric', 'gpa_grades']
                if any(col in df.columns for col in old_columns):
                    st.warning("‚ö†Ô∏è Detected old data format. Regenerating dataset...")
                    os.remove(file_path)  # Delete old file
                    should_generate = True
                else:
                    # Check if GPA column is numeric
                    if df['gpa'].dtype == 'object':
                        st.warning("‚ö†Ô∏è Detected old GPA format (string). Converting to 4.0 scale...")
                        gpa_mapping = {'P': 2.0, 'C': 2.5, 'D': 3.0, 'HD': 4.0}
                        df['gpa'] = df['gpa'].map(gpa_mapping).fillna(2.5)

                    # Ensure GPA is in correct range (2.0-4.0)
                    if df['gpa'].max() > 4.5:  # Old 10-point scale data
                        st.warning("‚ö†Ô∏è Detected 10-point GPA scale. Regenerating dataset...")
                        os.remove(file_path)
                        should_generate = True
                    else:
                        # Ensure all numeric columns are correct type
                        numeric_cols = ['gpa', 'leadership_score', 'communication_score',
                                        'self_efficacy', 'organisational_support', 'proba']
                        for col in numeric_cols:
                            if col in df.columns:
                                df[col] = pd.to_numeric(df[col], errors='coerce')

                        st.success(f"‚úÖ Loaded {len(df)} candidates from fair dataset")
        except Exception as e:
            st.error(f"‚ùå Error reading file: {str(e)}")
            should_generate = True
    else:
        st.warning("‚ö†Ô∏è Dataset not found, generating fair synthetic data...")
        should_generate = True

    if should_generate:
        # ============================================
        # ‚úÖ FAIR HIRING DATA GENERATION LOGIC
        # ============================================

        N_RECORDS = 5000
        CURRENT_YEAR = 2025
        np.random.seed(42)

        # 1. Cohort Year (graduation year)
        cohort_year = np.random.choice(
            [2019, 2020, 2021, 2022, 2023, 2024],
            N_RECORDS
        )

        # 2. Gender (gender ratio) - based on Australian real data, does not affect selection
        gender = np.random.choice(
            ['Female', 'Male', 'Non-binary'],
            N_RECORDS,
            p=[0.506, 0.492, 0.002]
        )

        # 3. Ethnicity - simplified to 11 main categories
        ethnicity_simplified = [
            'Australian', 'Aboriginal', 'Torres Strait Islander', 'British',
            'Indian', 'Chinese', 'New Zealand', 'Filipino',
            'Vietnamese', 'South African', 'Other'
        ]
        ethnicity_probs = [0.662, 0.019, 0.019, 0.035, 0.034,
                           0.026, 0.023, 0.015, 0.012, 0.008, 0.147]

        ethnicity = np.random.choice(ethnicity_simplified, N_RECORDS, p=ethnicity_probs)

        # 4. Degree Major
        degree_major = np.random.choice(
            ['Education', 'Commerce', 'STEM', 'Science', 'Arts', 'Other'],
            N_RECORDS,
            p=[0.35, 0.20, 0.10, 0.15, 0.15, 0.05]
        )

        # 5. Teaching Experience (automatically derived from graduation year)
        years_since_grad = CURRENT_YEAR - cohort_year

        def map_experience(years):
            if years <= 1:
                return '<1 year'
            elif years <= 3:
                return '1-3 years'
            else:
                return '3+ years'

        teaching_experience = pd.Series(years_since_grad).apply(map_experience).values

        # 6. GPA (using 4.0 scale)
        gpa_grades = np.random.choice(
            ['P', 'C', 'D', 'HD'],
            N_RECORDS,
            p=[0.25, 0.40, 0.25, 0.10]
        )

        # 4.0 scale mapping: P=2.0, C=2.5, D=3.0, HD=4.0
        gpa_mapping = {'P': 2.0, 'C': 2.5, 'D': 3.0, 'HD': 4.0}
        gpa_numeric = pd.Series(gpa_grades).map(gpa_mapping).values

        # 7. Assessment scores (1-10 scale)
        def generate_score(mean, std, size):
            score = np.random.normal(mean, std, size)
            return np.clip(score, 1.0, 10.0).round(1)

        communication_score = generate_score(7.0, 1.5, N_RECORDS)
        self_efficacy = generate_score(6.8, 1.8, N_RECORDS)
        leadership_score = generate_score(6.0, 2.0, N_RECORDS)
        org_support = generate_score(5.5, 2.5, N_RECORDS)

        # 8. Numeric experience
        exp_mapping = {'<1 year': 0, '1-3 years': 1, '3+ years': 2}
        exp_numeric = pd.Series(teaching_experience).map(exp_mapping).values

        # ============================================
        # ‚úÖ FAIR HIRING MODEL LOGIC
        # ============================================

        # Core improvement: remove major discrimination
        major_numeric = np.zeros(N_RECORDS)

        # Adjust weights: emphasize teaching-relevant skills
        # With perfect scores (GPA=4.0, all others=10.0, exp=1):
        # z = -6.5 + 1.2 + 4.5 + 2.5 + 0.8 + 2.5 + 1.2 = 6.2 ‚Üí prob ‚âà 99.8%
        intercept = -6.5  # Increased to allow high performers to succeed
        w_gpa = 0.3       # GPA weight (4.0 scale)
        w_comm = 0.45     # Communication (most important teaching skill)
        w_efficacy = 0.25 # Self-efficacy
        w_exp = 0.4       # Teaching experience (0-2 scale)
        w_leadership = 0.25 # Leadership
        w_org_support = 0.12 # Organizational support alignment
        w_major = 0.0     # Major does not affect selection

        epsilon = np.random.normal(0, 0.8, N_RECORDS)

        z = (intercept +
             w_gpa * gpa_numeric +
             w_comm * communication_score +
             w_efficacy * self_efficacy +
             w_exp * exp_numeric +
             w_leadership * leadership_score +
             w_org_support * org_support +
             w_major * major_numeric +
             epsilon)

        prob_hired = 1 / (1 + np.exp(-z))
        selected = np.random.binomial(1, prob_hired, N_RECORDS)

        # ============================================
        # BUILD FINAL DATAFRAME
        # ============================================
        df = pd.DataFrame({
            "candidate_id": np.arange(1, N_RECORDS + 1),
            "cohort": cohort_year.astype(int),
            "gender": gender,
            "ethnicity": ethnicity,
            "degree_major": degree_major,
            "teaching_experience": teaching_experience,
            "gpa": gpa_numeric.astype(float),  # Ensure it's float
            "leadership_score": leadership_score.astype(float),
            "communication_score": communication_score.astype(float),
            "self_efficacy": self_efficacy.astype(float),
            "organisational_support": org_support.astype(float),
            "selected": selected.astype(int),
            "proba": prob_hired.astype(float)
        })

        # ============================================
        # FAIRNESS VERIFICATION
        # ============================================
        print("\n" + "=" * 70)
        print("‚úÖ FAIR HIRING MODEL - DATA GENERATION REPORT")
        print("=" * 70)
        print(f"Total candidates: {N_RECORDS:,}")
        print(f"Overall selection rate: {selected.mean():.2%}")

        print("\nüîç FAIRNESS TESTS:")
        print("-" * 70)

        # Gender fairness
        gender_stats = df.groupby('gender')['selected'].agg(['mean', 'count'])
        print("\n1Ô∏è‚É£ Selection rate by gender:")
        for g in gender_stats.index:
            rate = gender_stats.loc[g, 'mean']
            count = gender_stats.loc[g, 'count']
            print(f"   {g:12s}: {rate:6.2%} (n={count:,})")
        gender_variance = gender_stats['mean'].std()
        print(f"   Std Dev: {gender_variance:.4f} {'‚úì Fair' if gender_variance < 0.02 else '‚ö†Ô∏è Disparity exists'}")

        # Ethnicity fairness
        eth_stats = df.groupby('ethnicity')['selected'].agg(['mean', 'count'])
        eth_stats = eth_stats[eth_stats['count'] >= 50].sort_values('mean', ascending=False)
        print("\n2Ô∏è‚É£ Selection rate by ethnicity (top 5):")
        for eth in eth_stats.head().index:
            rate = eth_stats.loc[eth, 'mean']
            count = eth_stats.loc[eth, 'count']
            print(f"   {eth:20s}: {rate:6.2%} (n={count:,})")
        eth_variance = eth_stats['mean'].std()
        print(f"   Std Dev: {eth_variance:.4f} {'‚úì Fair' if eth_variance < 0.03 else '‚ö†Ô∏è Disparity exists'}")

        # Major fairness
        major_stats = df.groupby('degree_major')['selected'].agg(['mean', 'count'])
        print("\n3Ô∏è‚É£ Selection rate by major:")
        for major in major_stats.sort_values('mean', ascending=False).index:
            rate = major_stats.loc[major, 'mean']
            count = major_stats.loc[major, 'count']
            print(f"   {major:12s}: {rate:6.2%} (n={count:,})")
        major_variance = major_stats['mean'].std()
        print(f"   Std Dev: {major_variance:.4f} {'‚úì Fair' if major_variance < 0.03 else '‚ö†Ô∏è Disparity exists'}")

        print("\n" + "=" * 70)
        print("‚ú® FAIRNESS SUMMARY")
        print("=" * 70)
        print("""
‚úÖ Improvement highlights:
  1. ‚ùå Removed major discrimination - all majors weighted at 0
  2. ‚¨ÜÔ∏è  Increased communication weight - 0.45 (core teaching skill)
  3. ‚ûï Added leadership score - 0.25 (classroom management)
  4. ‚ûï Added org support score - 0.12 (mission alignment)
  5. ‚¨ÜÔ∏è  Balanced GPA weight - 0.3 (important but not sole factor)
  6. üö´ Gender and ethnicity don't affect selection decisions
  7. üìä Selection rate controlled at reasonable level (15-25%)
  8. ‚ö° Adjusted intercept (-6.5) so top performers reach ~99.8% acceptance
  9. üéØ RF model uses class_weight={0:1, 1:3} to reduce false negatives

üéØ Core principle: Evaluate by individual ability, not by group labels

üìä Perfect Score Example (GPA=4.0, all scores=10, exp=1-3yr):
   z = -6.5 + 1.2 + 4.5 + 2.5 + 0.4 + 2.5 + 1.2 = 5.8
   Probability ‚âà 99.7% ‚úì (High performers should succeed!)
        """)

        df.to_csv(file_path, index=False, encoding="utf-8-sig")
        st.success(f"‚úÖ Fair dataset generated: {file_path}")

    # Model Card
    try:
        card = open("TFA_Model_Card.md").read()
    except FileNotFoundError:
        card = """# Model Card: TFA Fair Recruitment AI

## Model Details
- **Version**: 3.0 (Fair Version)
- **Type**: Binary Classification (Random Forest)
- **Purpose**: Assist in fair candidate selection for Teach For Australia

## Fair Hiring Principles
- **No demographic bias**: Gender, ethnicity removed from decision logic
- **No major discrimination**: All degree majors weighted equally (w=0)
- **Merit-based**: Focus on teaching-relevant skills

## Scoring System
- **GPA**: 4.0 scale (2.0=Pass, 2.5=Credit, 3.0=Distinction, 4.0=HD)
- **Other Scores**: 1-10 scale (Leadership, Communication, Self-Efficacy, Org Support)

## Weights (Logistic Regression Coefficients)
- GPA (4.0 scale): 0.3
- Communication (1-10): 0.45 (highest weight - core teaching skill)
- Self-Efficacy (1-10): 0.25
- Teaching Experience (0-2): 0.4
- Leadership (1-10): 0.25 (classroom management)
- Org Support (1-10): 0.12 (alignment with mission)
- Major: 0.0 (removed discrimination)
- Intercept: -6.5 (allows top performers ~99.8% acceptance rate)

## Model Training
- Algorithm: Random Forest Classifier
- Trees: 400
- Max Depth: Unlimited (for better fitting)
- Class Weight: {0:1.0, 1:3.0} (boost positive class to reduce false negatives)

## Ethical Considerations
- Demographic features used ONLY for fairness monitoring
- Regular bias audits conducted
- Human oversight required for all decisions
- Candidates informed of AI usage

## Limitations
- Trained on synthetic data (demo purposes)
- Requires retraining with real data
- Not a replacement for human judgment
"""
    return df, card


@st.cache_resource
def train_model(data):
    """Train Random Forest model"""
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import OneHotEncoder, StandardScaler
    from sklearn.compose import ColumnTransformer
    from sklearn.pipeline import Pipeline
    from sklearn.ensemble import RandomForestClassifier

    target = "selected"
    exclude_cols = [target, "candidate_id", "proba"]
    feature_cols = [c for c in data.columns if c not in exclude_cols]

    X = data[feature_cols].copy()
    y = data[target].astype(int)

    # Ensure numeric columns are correct data type
    numeric_cols_expected = ['gpa', 'leadership_score', 'communication_score',
                             'self_efficacy', 'organisational_support']
    for col in numeric_cols_expected:
        if col in X.columns:
            X[col] = pd.to_numeric(X[col], errors='coerce').fillna(X[col].median())

    potential_cat_cols = ["gender", "ethnicity", "degree_major", "teaching_experience", "cohort"]
    cat_cols = [col for col in potential_cat_cols if col in X.columns]
    num_cols = [c for c in feature_cols if c not in cat_cols]

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=42, stratify=y
    )

    transformers = []
    if cat_cols:
        transformers.append(
            ("cat", OneHotEncoder(handle_unknown="ignore", sparse_output=False), cat_cols)
        )
    if num_cols:
        transformers.append(
            ("num", StandardScaler(), num_cols)
        )

    preprocess = ColumnTransformer(transformers, remainder='drop')
    rf = RandomForestClassifier(
        n_estimators=400,
        max_depth=None,              # Remove depth limit for better fitting
        min_samples_split=20,        # Reduced from 50
        min_samples_leaf=10,         # Reduced from 20
        random_state=42,
        n_jobs=-1,
        class_weight={0: 1.0, 1: 3.0}  # ‚òÖ Boost positive class weight to reduce false negatives
    )

    model = Pipeline([("pre", preprocess), ("clf", rf)])
    model.fit(X_train, y_train)

    feature_names = []
    if cat_cols:
        cat_feature_names = list(
            model.named_steps['pre'].named_transformers_['cat']
            .get_feature_names_out(cat_cols)
        )
        feature_names.extend(cat_feature_names)
    if num_cols:
        feature_names.extend(num_cols)

    return model, feature_names, cat_cols, num_cols


data, model_card_text = load_data()

# Clear old model cache (if data format changed)
if 'last_data_hash' not in st.session_state:
    st.session_state.last_data_hash = None

current_data_hash = hash(tuple(data.columns))
if st.session_state.last_data_hash != current_data_hash:
    st.cache_resource.clear()  # Clear model cache
    st.session_state.last_data_hash = current_data_hash

model, feature_names, cat_cols, num_cols = train_model(data)

# =============================
# SIDEBAR
# =============================
st.sidebar.title("üéõÔ∏è Dashboard Controls")

# Clear cache button
if st.sidebar.button("üîÑ Clear Cache & Retrain", help="Clear model cache and retrain with current data"):
    st.cache_data.clear()
    st.cache_resource.clear()
    st.success("‚úÖ Cache cleared! Reloading...")
    st.rerun()

st.sidebar.markdown("---")

threshold = st.sidebar.slider(
    "Decision Threshold",
    0.0, 1.0, 0.5, 0.01,
    help="Adjust classification threshold. Perfect candidates (all 10s, GPA 4.0) typically reach ~99.7% probability."
)

st.sidebar.markdown("---")
st.sidebar.subheader("üîç Filters")

selected_cohorts = st.sidebar.multiselect(
    "Cohort Year",
    options=sorted(data["cohort"].unique()),
    default=sorted(data["cohort"].unique())
)

selected_majors = st.sidebar.multiselect(
    "Degree Major",
    options=sorted(data["degree_major"].unique()),
    default=sorted(data["degree_major"].unique())
)

selected_genders = st.sidebar.multiselect(
    "Gender",
    options=sorted(data["gender"].unique()),
    default=sorted(data["gender"].unique())
)

filtered_data = data[
    (data["cohort"].isin(selected_cohorts)) &
    (data["degree_major"].isin(selected_majors)) &
    (data["gender"].isin(selected_genders))
    ].copy()

filtered_data["pred"] = (filtered_data["proba"] >= threshold).astype(int)

st.sidebar.markdown("---")
st.sidebar.info(f"üìä **{len(filtered_data):,}** candidates selected")

csv = filtered_data.to_csv(index=False)
st.sidebar.download_button(
    label="üì• Download Data",
    data=csv,
    file_name="tfa_filtered_data.csv",
    mime="text/csv"
)


# =============================
# METRICS
# =============================
def calculate_metrics(df, threshold):
    y_true = df["selected"]
    y_pred = (df["proba"] >= threshold).astype(int)
    y_proba = df["proba"]

    auc = roc_auc_score(y_true, y_proba)
    accuracy = (y_pred == y_true).mean()
    cm = confusion_matrix(y_true, y_pred)
    tn, fp, fn, tp = cm.ravel()

    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

    return {"auc": auc, "accuracy": accuracy, "precision": precision,
            "recall": recall, "f1": f1, "tp": tp, "fp": fp, "tn": tn, "fn": fn}


metrics = calculate_metrics(filtered_data, threshold)

col1, col2, col3, col4, col5 = st.columns(5)
with col1:
    st.metric("üéØ AUC", f"{metrics['auc']:.3f}")
with col2:
    st.metric("‚úÖ Accuracy", f"{metrics['accuracy']:.3f}")
with col3:
    st.metric("üé™ Precision", f"{metrics['precision']:.3f}")
with col4:
    st.metric("üìã Recall", f"{metrics['recall']:.3f}")
with col5:
    st.metric("‚öñÔ∏è F1", f"{metrics['f1']:.3f}")

st.markdown("---")

# =============================
# TABBED INTERFACE
# =============================
tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
    "üé≤ Candidate Simulator",
    "üìà Performance",
    "‚öñÔ∏è Fairness Analysis",
    "üéØ Calibration",
    "üî¨ Feature Analysis",
    "üìÑ Model Card"
])

# =============================
# TAB 1: CANDIDATE SIMULATOR
# =============================
with tab1:
    st.header("üé≤ Interactive Candidate Simulator")
    st.markdown("Choose between uploading a resume or manually building a candidate profile.")

    # Create two completely independent subtabs
    subtab1, subtab2 = st.tabs(["üìÑ Upload Resume (AI Parser)", "‚úèÔ∏è Manual Profile Builder"])

    # =============================
    # SUB-TAB 1: PDF UPLOAD
    # =============================
    with subtab1:
        st.markdown("### üìÑ Upload Resume for Automatic Analysis")
        st.info("üí° Upload a PDF resume and let AI extract candidate information automatically.")

        col_upload1, col_upload2 = st.columns([3, 1])

        with col_upload1:
            uploaded_file = st.file_uploader(
                "Upload candidate's resume (PDF format)",
                type=['pdf'],
                help="Upload a PDF resume for automatic analysis",
                key="pdf_uploader"
            )

            if uploaded_file is not None:
                file_details = {
                    "Filename": uploaded_file.name,
                    "FileType": uploaded_file.type,
                    "FileSize": f"{uploaded_file.size / 1024:.2f} KB"
                }
                st.caption(f"üìÑ {file_details['Filename']} ({file_details['FileSize']})")

        with col_upload2:
            use_llm = st.checkbox("ü§ñ Use AI Parser", value=True,
                                  help="Use Claude AI for better accuracy (requires API key)",
                                  key="use_llm_checkbox")
            st.markdown("<br>", unsafe_allow_html=True)
            if uploaded_file is not None:
                analyze_btn = st.button("üîç Analyze Resume", type="primary", use_container_width=True)
            else:
                st.button("üîç Analyze Resume", type="primary", use_container_width=True, disabled=True)

        if uploaded_file is not None and analyze_btn:
            with st.spinner("Analyzing resume..."):
                uploaded_file.seek(0)
                parsed_data = parse_resume_hybrid(uploaded_file, use_llm=use_llm)

                if parsed_data:
                    st.session_state.pdf_candidate = parsed_data  # Use independent key
                    st.success("‚úÖ Resume parsed successfully!")

        # Display parsed PDF candidate information
        if 'pdf_candidate' in st.session_state:
            c = st.session_state.pdf_candidate

            with st.expander("üìã Extracted Information", expanded=True):
                col1, col2, col3 = st.columns(3)

                with col1:
                    st.markdown("**üìö Academic**")
                    st.write(f"- GPA: {c['gpa']:.2f}/4.0")
                    st.write(f"- Major: {c['degree_major']}")
                    st.write(f"- Experience: {c['teaching_experience']}")

                with col2:
                    st.markdown("**üéØ Scores (1-10)**")
                    st.write(f"- Leadership: {c['leadership_score']:.1f}")
                    st.write(f"- Communication: {c['communication_score']:.1f}")
                    st.write(f"- Self-Efficacy: {c['self_efficacy']:.1f}")

                with col3:
                    st.markdown("**üë§ Demographics**")
                    st.write(f"- Gender: {c['gender']}")
                    st.write(f"- Ethnicity: {c['ethnicity']}")
                    st.write(f"- Cohort: {c['cohort']}")

                if 'raw_text' in c:
                    st.markdown("**üìÑ Resume Preview:**")
                    st.text(c['raw_text'][:400] + "...")

            st.markdown("---")

            # Prediction results for PDF candidate
            col_result1, col_result2 = st.columns([1, 1.5])

            with col_result1:
                st.subheader("üéØ AI Prediction")

                candidate_df = pd.DataFrame({
                    "cohort": [c['cohort']],
                    "gender": [c['gender']],
                    "ethnicity": [c['ethnicity']],
                    "degree_major": [c['degree_major']],
                    "gpa": [c['gpa']],
                    "teaching_experience": [c['teaching_experience']],
                    "leadership_score": [c['leadership_score']],
                    "communication_score": [c['communication_score']],
                    "self_efficacy": [c['self_efficacy']],
                    "organisational_support": [c['organisational_support']]
                })

                proba = model.predict_proba(candidate_df)[0, 1]
                decision = "‚úÖ FAST-TRACK" if proba >= threshold else "üîÑ SECOND-LOOK"
                decision_class = "fasttrack-box" if proba >= threshold else "secondlook-box"

                st.markdown(f'<div class="{decision_class}">{decision}</div>', unsafe_allow_html=True)

                fig_gauge = go.Figure(go.Indicator(
                    mode="gauge+number+delta",
                    value=proba,
                    title={'text': "Probability", 'font': {'size': 16}},
                    delta={'reference': threshold},
                    number={'font': {'size': 30}, 'valueformat': '.3f'},
                    gauge={
                        'axis': {'range': [0, 1]},
                        'bar': {'color': "darkblue"},
                        'steps': [
                            {'range': [0, threshold], 'color': '#ffdddd'},
                            {'range': [threshold, 1], 'color': '#ddffdd'}
                        ],
                        'threshold': {'line': {'color': "red", 'width': 4}, 'value': threshold}
                    }
                ))
                fig_gauge.update_layout(height=220, margin=dict(l=10, r=10, t=50, b=10))
                st.plotly_chart(fig_gauge, use_container_width=True)

            with col_result2:
                st.subheader("üìä Score Breakdown")

                scores = {
                    'GPA (√ó25)': c['gpa'] * 25,
                    'Leadership': c['leadership_score'] * 10,
                    'Communication': c['communication_score'] * 10,
                    'Self-Efficacy': c['self_efficacy'] * 10,
                    'Org Support': c['organisational_support'] * 10
                }

                scores_df = pd.DataFrame({
                    'Metric': list(scores.keys()),
                    'Score': list(scores.values())
                })

                fig_scores = go.Figure()
                fig_scores.add_trace(go.Bar(
                    x=scores_df['Score'], y=scores_df['Metric'], orientation='h',
                    marker=dict(color=scores_df['Score'], colorscale='RdYlGn', cmin=0, cmax=100),
                    text=scores_df['Score'].round(1), textposition='outside'
                ))
                fig_scores.update_layout(height=300, xaxis=dict(range=[0, 105]), showlegend=False,
                                         title="Normalized Scores (0-100)")
                st.plotly_chart(fig_scores, use_container_width=True)

            st.markdown(f"""
            <div class="info-box">
            <h4>üìã Recommendation</h4>
            <ul>
            <li><b>Probability:</b> {proba * 100:.1f}%</li>
            <li><b>Threshold:</b> {threshold * 100:.0f}%</li>
            <li><b>Status:</b> {"‚úÖ Above threshold" if proba >= threshold else "‚ö†Ô∏è Below threshold"}</li>
            <li><b>Action:</b> {"Fast-track to interview stage" if proba >= threshold else "Schedule review panel discussion"}</li>
            </ul>
            </div>
            """, unsafe_allow_html=True)

    # =============================
    # SUB-TAB 2: MANUAL INPUT
    # =============================
    with subtab2:
        st.markdown("### ‚úèÔ∏è Manual Profile Builder")
        st.info("üí° Build a candidate profile manually by adjusting the parameters below.")

        col_input, col_output = st.columns([1.2, 1])

        with col_input:
            col_btn1, col_btn2 = st.columns(2)

            with col_btn1:
                if st.button("üé≤ Generate Random", use_container_width=True):
                    st.session_state.candidate = {
                        'cohort': int(np.random.choice([2019, 2020, 2021, 2022, 2023, 2024])),
                        'gender': np.random.choice(GENDERS),
                        'ethnicity': np.random.choice(ETHNICITIES_LIST),
                        'degree_major': np.random.choice(DEGREE_MAJORS),
                        'gpa': round(np.random.uniform(2.0, 4.0), 1),
                        'teaching_experience': np.random.choice(TEACHING_EXPERIENCE),
                        'leadership_score': round(np.random.uniform(4.0, 9.5), 1),
                        'communication_score': round(np.random.uniform(5.0, 9.5), 1),
                        'self_efficacy': round(np.random.uniform(5.0, 9.0), 1),
                        'organisational_support': round(np.random.uniform(4.0, 9.0), 1)
                    }

            with col_btn2:
                if st.button("üîÑ Reset to Default", use_container_width=True):
                    st.session_state.candidate = {
                        'cohort': 2023, 'gender': "Female", 'ethnicity': "Australian",
                        'degree_major': "Education", 'gpa': 2.5, 'teaching_experience': "1-3 years",
                        'leadership_score': 6.5, 'communication_score': 7.0,
                        'self_efficacy': 6.8, 'organisational_support': 5.5
                    }

            if 'candidate' not in st.session_state:
                st.session_state.candidate = {
                    'cohort': 2023, 'gender': "Female", 'ethnicity': "Australian",
                    'degree_major': "Education", 'gpa': 2.5, 'teaching_experience': "1-3 years",
                    'leadership_score': 6.5, 'communication_score': 7.0,
                    'self_efficacy': 6.8, 'organisational_support': 5.5
                }

            c = st.session_state.candidate

            st.markdown("#### üë§ Demographics (for fairness monitoring only)")
            col1, col2 = st.columns(2)
            with col1:
                gender = st.selectbox("Gender", GENDERS, index=GENDERS.index(c['gender']))
            with col2:
                current_ethnicity = c['ethnicity'] if c['ethnicity'] in ETHNICITIES_LIST else "Australian"
                ethnicity = st.selectbox("Ethnicity", ETHNICITIES_LIST,
                                         index=ETHNICITIES_LIST.index(current_ethnicity))

            st.markdown("#### üéì Academic Background")
            col1, col2 = st.columns(2)
            with col1:
                cohort = st.selectbox("Cohort Year", [2019, 2020, 2021, 2022, 2023, 2024],
                                      index=[2019, 2020, 2021, 2022, 2023, 2024].index(c['cohort']))
                major = st.selectbox("Degree Major", DEGREE_MAJORS,
                                     index=DEGREE_MAJORS.index(c['degree_major']))
            with col2:
                gpa = st.slider("GPA (4.0 scale)", 2.0, 4.0, float(c['gpa']), 0.1)
                experience = st.selectbox("Teaching Experience", TEACHING_EXPERIENCE,
                                          index=TEACHING_EXPERIENCE.index(c['teaching_experience']))

            st.markdown("#### üìä Assessment Scores (1-10 scale)")
            leadership = st.slider("Leadership Score", 1.0, 10.0, float(c['leadership_score']), 0.1)
            communication = st.slider("Communication Score", 1.0, 10.0, float(c['communication_score']), 0.1)
            efficacy = st.slider("Self-Efficacy Score", 1.0, 10.0, float(c['self_efficacy']), 0.1)
            support = st.slider("Organizational Support", 1.0, 10.0, float(c['organisational_support']), 0.1)

        with col_output:
            st.subheader("üéØ AI Prediction Results")

            candidate_df = pd.DataFrame({
                "cohort": [cohort],
                "gender": [gender],
                "ethnicity": [ethnicity],
                "degree_major": [major],
                "gpa": [gpa],
                "teaching_experience": [experience],
                "leadership_score": [leadership],
                "communication_score": [communication],
                "self_efficacy": [efficacy],
                "organisational_support": [support]
            })

            proba = model.predict_proba(candidate_df)[0, 1]
            decision = "‚úÖ FAST-TRACK" if proba >= threshold else "üîÑ SECOND-LOOK"
            decision_class = "fasttrack-box" if proba >= threshold else "secondlook-box"

            st.markdown(f'<div class="{decision_class}">{decision}</div>', unsafe_allow_html=True)

            fig_gauge = go.Figure(go.Indicator(
                mode="gauge+number+delta",
                value=proba,
                title={'text': "Selection Probability", 'font': {'size': 18}},
                delta={'reference': threshold},
                number={'font': {'size': 35}, 'valueformat': '.3f'},
                gauge={
                    'axis': {'range': [0, 1]},
                    'bar': {'color': "darkblue"},
                    'steps': [
                        {'range': [0, threshold], 'color': '#ffdddd'},
                        {'range': [threshold, 1], 'color': '#ddffdd'}
                    ],
                    'threshold': {'line': {'color': "red", 'width': 4}, 'value': threshold}
                }
            ))
            fig_gauge.update_layout(height=250, margin=dict(l=10, r=10, t=60, b=10))
            st.plotly_chart(fig_gauge, use_container_width=True)

            st.markdown(f"""
            <div class="info-box">
            <h4>üìã Results</h4>
            <ul>
            <li><b>Probability:</b> {proba * 100:.1f}%</li>
            <li><b>Threshold:</b> {threshold * 100:.0f}%</li>
            <li><b>Status:</b> {"‚úÖ Above threshold" if proba >= threshold else "‚ö†Ô∏è Below threshold"}</li>
            <li><b>Recommendation:</b> {"Fast-track to next stage" if proba >= threshold else "Schedule review panel"}</li>
            </ul>
            </div>
            """, unsafe_allow_html=True)

    st.markdown("---")
    st.subheader("üîç Feature Importance Analysis")
    st.info("üí° This shows which features are most important to the model's decisions overall")

    col1, col2 = st.columns([1.5, 1])

    with col1:
        importances = model.named_steps['clf'].feature_importances_
        importance_df = pd.DataFrame({
            'Feature': feature_names,
            'Importance': importances
        }).sort_values('Importance', ascending=False).head(10)

        def simplify_name(name):
            replacements = {
                'gender_': '', 'ethnicity_': '', 'degree_major_': '',
                'teaching_experience_': '', 'cohort_': '',
                'leadership_score': 'Leadership', 'communication_score': 'Communication',
                'self_efficacy': 'Self-Efficacy', 'organisational_support': 'Org Support',
                'gpa': 'GPA'
            }
            for old, new in replacements.items():
                name = name.replace(old, new)
            return name.title()

        importance_df['Feature'] = importance_df['Feature'].apply(simplify_name)

        fig_imp = px.bar(
            importance_df, x='Importance', y='Feature', orientation='h',
            title="Top 10 Most Important Features",
            color='Importance', color_continuous_scale='Viridis'
        )
        fig_imp.update_layout(height=400, showlegend=False)
        st.plotly_chart(fig_imp, use_container_width=True)

    with col2:
        st.markdown("### üìä Sample Candidate Scores")
        st.caption("Showing manual input candidate if available")

        # Use manual input candidate data
        if 'candidate' in st.session_state:
            mc = st.session_state.candidate
            scores = {
                'Leadership': mc['leadership_score'],
                'Communication': mc['communication_score'],
                'Self-Efficacy': mc['self_efficacy'],
                'Org Support': mc['organisational_support'],
                'GPA (√ó25)': mc['gpa'] * 25  # Normalize to 100 scale for display
            }
        else:
            scores = {
                'Leadership': 6.5,
                'Communication': 7.0,
                'Self-Efficacy': 6.8,
                'Org Support': 5.5,
                'GPA (√ó25)': 2.5 * 25
            }

        scores_df = pd.DataFrame({
            'Metric': list(scores.keys()),
            'Score': list(scores.values())
        })

        fig_scores = go.Figure()
        fig_scores.add_trace(go.Bar(
            x=scores_df['Score'], y=scores_df['Metric'], orientation='h',
            marker=dict(color=scores_df['Score'], colorscale='RdYlGn', cmin=0, cmax=100),
            text=scores_df['Score'].round(1), textposition='outside'
        ))
        fig_scores.update_layout(height=400, xaxis=dict(range=[0, 105]), showlegend=False,
                                 title="Normalized Scores (0-100)")
        st.plotly_chart(fig_scores, use_container_width=True)

# =============================
# TAB 2: PERFORMANCE
# =============================
with tab2:
    col1, col2 = st.columns(2)

    with col1:
        st.subheader("ROC Curve")
        fpr, tpr, thresholds = roc_curve(filtered_data["selected"], filtered_data["proba"])

        fig_roc = go.Figure()
        fig_roc.add_trace(go.Scatter(
            x=fpr, y=tpr, mode='lines', name=f'ROC (AUC={metrics["auc"]:.3f})',
            line=dict(color='#1f77b4', width=3), fill='tozeroy'
        ))
        fig_roc.add_trace(go.Scatter(
            x=[0, 1], y=[0, 1], mode='lines', name='Random',
            line=dict(color='gray', dash='dash')
        ))
        fig_roc.update_layout(xaxis_title="FPR", yaxis_title="TPR", height=400)
        st.plotly_chart(fig_roc, use_container_width=True)

    with col2:
        st.subheader("Precision-Recall Curve")
        precision, recall, _ = precision_recall_curve(filtered_data["selected"], filtered_data["proba"])

        fig_pr = go.Figure()
        fig_pr.add_trace(go.Scatter(
            x=recall, y=precision, mode='lines', name='PR Curve',
            line=dict(color='#ff7f0e', width=3), fill='tozeroy'
        ))
        fig_pr.update_layout(xaxis_title="Recall", yaxis_title="Precision", height=400)
        st.plotly_chart(fig_pr, use_container_width=True)

    st.subheader("üé≠ Confusion Matrix")

    col1, col2 = st.columns([1, 2])

    with col1:
        st.markdown(f"""
        **True Negatives:** {metrics['tn']}  
        **False Positives:** {metrics['fp']}  
        **False Negatives:** {metrics['fn']}  
        **True Positives:** {metrics['tp']}
        """)

    with col2:
        cm = confusion_matrix(filtered_data["selected"], filtered_data["pred"])
        fig_cm = px.imshow(
            cm, labels=dict(x="Predicted", y="Actual"),
            x=['Not Selected', 'Selected'], y=['Not Selected', 'Selected'],
            text_auto=True, color_continuous_scale='Blues'
        )
        fig_cm.update_layout(height=350)
        st.plotly_chart(fig_cm, use_container_width=True)

# =============================
# TAB 3: FAIRNESS ANALYSIS
# =============================
with tab3:
    def group_fairness_metrics(df, sensitive_col, ref_group):
        results = []
        for group in df[sensitive_col].unique():
            group_df = df[df[sensitive_col] == group]
            y_true = group_df["selected"]
            y_pred = group_df["pred"]

            selection_rate = y_pred.mean()

            if y_true.sum() > 0:
                tp = ((y_pred == 1) & (y_true == 1)).sum()
                fn = ((y_pred == 0) & (y_true == 1)).sum()
                tpr = tp / (tp + fn) if (tp + fn) > 0 else 0
            else:
                tpr = 0

            results.append({
                "Group": group,
                "Count": len(group_df),
                "Selection Rate": selection_rate,
                "TPR": tpr
            })

        df_results = pd.DataFrame(results)

        if ref_group in df_results["Group"].values:
            ref_row = df_results[df_results["Group"] == ref_group].iloc[0]
            df_results["SPD"] = df_results["Selection Rate"] - ref_row["Selection Rate"]
            df_results["TPR Gap"] = df_results["TPR"] - ref_row["TPR"]

        return df_results

    st.subheader("üë• Gender Fairness")
    gender_fairness = group_fairness_metrics(filtered_data, "gender", "Male")

    col1, col2 = st.columns([1, 2])

    with col1:
        st.dataframe(
            gender_fairness.style.format({
                "Selection Rate": "{:.1%}",
                "TPR": "{:.3f}",
                "SPD": "{:.3f}",
                "TPR Gap": "{:.3f}"
            }),
            height=200
        )

    with col2:
        fig_gender = px.bar(
            gender_fairness, x="Group", y=["SPD", "TPR Gap"],
            barmode='group', title="Fairness Gaps by Gender"
        )
        fig_gender.add_hline(y=0, line_dash="dash")
        fig_gender.update_layout(height=350)
        st.plotly_chart(fig_gender, use_container_width=True)

    st.markdown("---")

    st.subheader("üåç Ethnicity Fairness (Top Groups)")
    eth_fairness = group_fairness_metrics(filtered_data, "ethnicity", "Australian")
    eth_fairness_filtered = eth_fairness[eth_fairness["Count"] >= 10].sort_values("Count", ascending=False).head(10)

    col1, col2 = st.columns([1, 2])

    with col1:
        st.dataframe(
            eth_fairness_filtered.style.format({
                "Selection Rate": "{:.1%}",
                "TPR": "{:.3f}",
                "SPD": "{:.3f}",
                "TPR Gap": "{:.3f}"
            }),
            height=400
        )

    with col2:
        fig_eth = px.bar(
            eth_fairness_filtered, x="Group", y=["SPD", "TPR Gap"],
            barmode='group', title="Fairness Gaps by Ethnicity"
        )
        fig_eth.add_hline(y=0, line_dash="dash")
        fig_eth.update_layout(height=400, xaxis_tickangle=-45)
        st.plotly_chart(fig_eth, use_container_width=True)

# =============================
# TAB 4: CALIBRATION
# =============================
with tab4:
    st.subheader("üéØ Model Calibration")

    col1, col2 = st.columns(2)

    with col1:
        fraction_of_positives, mean_predicted_value = calibration_curve(
            filtered_data["selected"], filtered_data["proba"], n_bins=10
        )

        fig_calib = go.Figure()
        fig_calib.add_trace(go.Scatter(
            x=mean_predicted_value, y=fraction_of_positives,
            mode='lines+markers', name='Model', line=dict(width=3)
        ))
        fig_calib.add_trace(go.Scatter(
            x=[0, 1], y=[0, 1], mode='lines', name='Perfect',
            line=dict(dash='dash', color='gray')
        ))
        fig_calib.update_layout(
            title="Calibration Curve",
            xaxis_title="Predicted Probability",
            yaxis_title="Fraction of Positives",
            height=400
        )
        st.plotly_chart(fig_calib, use_container_width=True)

    with col2:
        st.markdown("### üéØ Risk Bands")

        def triage_band(p):
            if p < 0.3:
                return "Low"
            elif p < 0.5:
                return "Med-Low"
            elif p < 0.7:
                return "Med-High"
            else:
                return "High"

        filtered_data["band"] = filtered_data["proba"].apply(triage_band)
        triage_summary = filtered_data.groupby("band").agg({
            "candidate_id": "count",
            "selected": "mean"
        }).reset_index()
        triage_summary.columns = ["Band", "Count", "Actual Rate"]

        st.dataframe(triage_summary.style.format({"Actual Rate": "{:.1%}"}), hide_index=True)

        fig_bands = px.bar(
            triage_summary, x="Band", y="Count", color="Actual Rate",
            color_continuous_scale="RdYlGn"
        )
        fig_bands.update_layout(height=300)
        st.plotly_chart(fig_bands, use_container_width=True)

# =============================
# TAB 5: FEATURE ANALYSIS
# =============================
with tab5:
    st.subheader("üî¨ Feature Distribution Analysis")

    numeric_features = ["gpa", "leadership_score", "communication_score", "self_efficacy", "organisational_support"]
    selected_feature = st.selectbox("Select feature:", numeric_features)

    col1, col2 = st.columns(2)

    with col1:
        fig_dist = go.Figure()
        fig_dist.add_trace(go.Violin(
            y=filtered_data[filtered_data['selected'] == 1][selected_feature],
            name='Selected', box_visible=True, fillcolor='lightgreen'
        ))
        fig_dist.add_trace(go.Violin(
            y=filtered_data[filtered_data['selected'] == 0][selected_feature],
            name='Not Selected', box_visible=True, fillcolor='lightcoral'
        ))
        fig_dist.update_layout(title=f"{selected_feature} by Outcome", height=400)
        st.plotly_chart(fig_dist, use_container_width=True)

    with col2:
        fig_gender = px.box(
            filtered_data, x="gender", y=selected_feature,
            color="gender", title=f"{selected_feature} by Gender"
        )
        fig_gender.update_layout(height=400, showlegend=False)
        st.plotly_chart(fig_gender, use_container_width=True)

    st.markdown("---")
    st.subheader("üìà Feature Correlations")

    corr_features = numeric_features + ['proba', 'selected']
    corr_matrix = filtered_data[corr_features].corr()

    fig_corr = px.imshow(
        corr_matrix, text_auto='.2f',
        color_continuous_scale='RdBu_r', zmin=-1, zmax=1
    )
    fig_corr.update_layout(height=500)
    st.plotly_chart(fig_corr, use_container_width=True)

# =============================
# TAB 6: MODEL CARD
# =============================
with tab6:
    st.markdown(model_card_text)

    st.markdown("---")

    col1, col2, col3 = st.columns(3)

    with col1:
        st.markdown("""
        ### üìã Model Info
        - **Type**: Random Forest
        - **Version**: 3.0 (Fair)
        - **Trees**: 400 (high capacity)
        - **Class Weight**: 1:3 (favor positives)
        - **Parser**: Claude AI + Keywords
        - **GPA Scale**: 4.0 (US standard)
        - **Other Scores**: 1-10 scale
        """)

    with col2:
        st.markdown(f"""
        ### üéØ Performance
        - **AUC**: {metrics['auc']:.3f}
        - **Accuracy**: {metrics['accuracy']:.3f}
        - **F1**: {metrics['f1']:.3f}
        """)

    with col3:
        st.markdown("""
        ### ‚öñÔ∏è Fairness
        - ‚ùå No major bias (w=0)
        - üö´ Demographics excluded
        - ‚úÖ Merit-based only
        """)

    st.markdown("---")

    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Total Candidates", f"{len(data):,}")
    with col2:
        st.metric("Ethnic Groups", f"{data['ethnicity'].nunique()}")
    with col3:
        st.metric("Selection Rate", f"{data['selected'].mean():.1%}")

# =============================
# FOOTER
# =============================
st.markdown("---")
st.markdown(f"""
<div style='text-align: center; color: gray; padding: 1rem;'>
    <p>üéì TFA Fair AI Dashboard v3.0 | Powered by Claude AI ü§ñ</p>
    <p>‚úÖ Fair Hiring Model: No bias by demographics | Merit-based evaluation</p>
    <p>üìä GPA: 4.0 scale | Other scores: 1-10 scale</p>
    <p><i>Install: pip install streamlit pandas numpy plotly scikit-learn pymupdf4llm anthropic PyPDF2</i></p>
    <p><i>‚ö†Ô∏è Remember to set ANTHROPIC_API_KEY as environment variable for AI parsing</i></p>
</div>
""", unsafe_allow_html=True)
